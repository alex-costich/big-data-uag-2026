{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verificaci√≥n del Entorno\n",
    "\n",
    "## Objetivos de Aprendizaje\n",
    "- Verificar que el entorno de desarrollo est√° correctamente configurado\n",
    "- Comprobar la conexi√≥n con el cluster de Spark\n",
    "- Familiarizarse con Jupyter Lab\n",
    "\n",
    "## Prerequisitos\n",
    "- Docker Desktop corriendo\n",
    "- Cluster iniciado con:\n",
    "  - **macOS/Linux:** `./infrastructure/scripts/start-cluster.sh spark`\n",
    "  - **Windows:** `.\\infrastructure\\scripts\\start-cluster.ps1 spark`\n",
    "  - **Alternativa (todas las plataformas):** `docker compose -f infrastructure/docker-compose.spark.yml up -d --build`\n",
    "\n",
    "## Tiempo Estimado\n",
    "‚è±Ô∏è 15 minutos\n",
    "\n",
    "## M√≥dulo AWS Academy Relacionado\n",
    "üìö Este notebook es preparaci√≥n para todos los m√≥dulos del curso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# === SECCI√ìN 1 ===\n",
    "## 1. Verificaci√≥n de Python\n",
    "\n",
    "### Explicaci√≥n Conceptual\n",
    "Python es el lenguaje principal que usaremos. Piensa en Python como el \"idioma\" que usamos para comunicarnos con las herramientas de Big Data. Necesitamos verificar que Python est√° instalado y funcionando correctamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos el modulo sys que nos da informacion del sistema\n",
    "import sys\n",
    "\n",
    "# Mostramos la version de Python instalada\n",
    "print(f\"Version de Python: {sys.version}\")\n",
    "\n",
    "# Mostramos la ubicacion del ejecutable de Python\n",
    "print(f\"Ubicacion: {sys.executable}\")\n",
    "\n",
    "# Output esperado:\n",
    "# Version de Python: 3.11.x (o superior)\n",
    "# Ubicacion: /opt/conda/bin/python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úÖ Verificaci√≥n\n",
    "Si ves la versi√≥n de Python 3.11 o superior, ¬°Python est√° funcionando correctamente!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# === SECCI√ìN 2 ===\n",
    "## 2. Verificaci√≥n de PySpark\n",
    "\n",
    "### Explicaci√≥n Conceptual\n",
    "PySpark es la interfaz de Python para Apache Spark. Imagina que Spark es un equipo de trabajadores muy eficientes que pueden procesar grandes cantidades de datos. PySpark es el \"traductor\" que nos permite dar instrucciones a ese equipo usando Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos pyspark para verificar que esta instalado\n",
    "import pyspark\n",
    "\n",
    "# Mostramos la version de PySpark\n",
    "print(f\"Version de PySpark: {pyspark.__version__}\")\n",
    "\n",
    "# Output esperado:\n",
    "# Version de PySpark: 3.5.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# === SECCI√ìN 3 ===\n",
    "## 3. Crear una Sesi√≥n de Spark\n",
    "\n",
    "### Explicaci√≥n Conceptual\n",
    "Una \"sesi√≥n de Spark\" es como abrir una aplicaci√≥n en tu computadora. Antes de poder usar Spark, necesitamos \"iniciar\" una sesi√≥n. Esta sesi√≥n es nuestra conexi√≥n con el cluster de Spark que procesa los datos.\n",
    "\n",
    "**Analog√≠a del mundo real:** Es como encender un auto antes de conducir. La sesi√≥n de Spark es el \"encendido\" que nos permite usar todas las capacidades del motor (cluster)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos SparkSession, la clase principal para crear sesiones\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Creamos una sesion de Spark\n",
    "# .builder inicia el proceso de construccion de la sesion\n",
    "# .appName() le da un nombre a nuestra aplicacion (aparece en la UI de Spark)\n",
    "# .getOrCreate() crea una nueva sesion o reutiliza una existente\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"EnvironmentCheck\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Mostramos informacion de la sesion creada\n",
    "print(f\"Sesion de Spark creada exitosamente!\")\n",
    "print(f\"Version de Spark: {spark.version}\")\n",
    "print(f\"Aplicacion: {spark.sparkContext.appName}\")\n",
    "\n",
    "# Output esperado:\n",
    "# Sesion de Spark creada exitosamente!\n",
    "# Version de Spark: 3.5.0\n",
    "# Aplicacion: EnvironmentCheck"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üí° Tip\n",
    "Una vez creada la sesi√≥n, puedes ver tu aplicaci√≥n en la Spark UI: http://localhost:4040"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# === SECCI√ìN 4 ===\n",
    "## 4. Crear un DataFrame de Prueba\n",
    "\n",
    "### Explicaci√≥n Conceptual\n",
    "Un DataFrame es como una tabla de Excel: tiene filas y columnas. En Spark, los DataFrames pueden contener millones o billones de filas y se procesan de forma distribuida (en m√∫ltiples computadoras a la vez).\n",
    "\n",
    "**Analog√≠a del mundo real:** Si tienes que contar monedas, un DataFrame te permite dividir las monedas entre varios amigos para contar m√°s r√°pido, y luego sumar los resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos una lista de datos de ejemplo\n",
    "# Cada tupla representa una fila: (nombre, edad, ciudad)\n",
    "datos = [\n",
    "    (\"Ana\", 25, \"CDMX\"),\n",
    "    (\"Carlos\", 30, \"Guadalajara\"),\n",
    "    (\"Maria\", 28, \"Monterrey\"),\n",
    "    (\"Juan\", 35, \"CDMX\"),\n",
    "    (\"Laura\", 22, \"Guadalajara\")\n",
    "]\n",
    "\n",
    "# Definimos los nombres de las columnas\n",
    "columnas = [\"nombre\", \"edad\", \"ciudad\"]\n",
    "\n",
    "# Creamos el DataFrame usando spark.createDataFrame()\n",
    "# El primer argumento son los datos\n",
    "# El segundo argumento son los nombres de las columnas\n",
    "df = spark.createDataFrame(datos, columnas)\n",
    "\n",
    "# Mostramos el contenido del DataFrame\n",
    "# show() muestra las primeras filas en formato tabla\n",
    "print(\"Contenido del DataFrame:\")\n",
    "df.show()\n",
    "\n",
    "# Output esperado:\n",
    "# +------+----+-----------+\n",
    "# |nombre|edad|     ciudad|\n",
    "# +------+----+-----------+\n",
    "# |   Ana|  25|       CDMX|\n",
    "# |Carlos|  30|Guadalajara|\n",
    "# | Maria|  28|  Monterrey|\n",
    "# |  Juan|  35|       CDMX|\n",
    "# | Laura|  22|Guadalajara|\n",
    "# +------+----+-----------+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostramos el esquema del DataFrame\n",
    "# El esquema describe la estructura: nombre y tipo de cada columna\n",
    "print(\"Esquema del DataFrame:\")\n",
    "df.printSchema()\n",
    "\n",
    "# Output esperado:\n",
    "# root\n",
    "#  |-- nombre: string (nullable = true)\n",
    "#  |-- edad: long (nullable = true)\n",
    "#  |-- ciudad: string (nullable = true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contamos el numero de filas\n",
    "# count() es una \"accion\" que ejecuta el calculo y retorna un numero\n",
    "num_filas = df.count()\n",
    "print(f\"Numero de filas: {num_filas}\")\n",
    "\n",
    "# Contamos el numero de columnas\n",
    "# len(df.columns) nos da la cantidad de columnas\n",
    "num_columnas = len(df.columns)\n",
    "print(f\"Numero de columnas: {num_columnas}\")\n",
    "\n",
    "# Output esperado:\n",
    "# Numero de filas: 5\n",
    "# Numero de columnas: 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# === SECCI√ìN 5 ===\n",
    "## 5. Verificaci√≥n de Librer√≠as de Data Science\n",
    "\n",
    "### Explicaci√≥n Conceptual\n",
    "Adem√°s de Spark, usaremos librer√≠as populares de Python para an√°lisis y visualizaci√≥n de datos. Estas librer√≠as complementan a Spark para tareas espec√≠ficas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos las librerias principales de data science\n",
    "import pandas as pd        # Analisis de datos (DataFrames locales)\n",
    "import numpy as np         # Operaciones numericas\n",
    "import matplotlib.pyplot as plt  # Graficas basicas\n",
    "\n",
    "# Mostramos las versiones de cada libreria\n",
    "print(f\"Pandas: {pd.__version__}\")\n",
    "print(f\"NumPy: {np.__version__}\")\n",
    "print(f\"Matplotlib: {plt.matplotlib.__version__}\")\n",
    "\n",
    "# Output esperado:\n",
    "# Pandas: 2.x.x\n",
    "# NumPy: 1.x.x\n",
    "# Matplotlib: 3.x.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificamos que Delta Lake esta disponible\n",
    "# Delta Lake es un formato de almacenamiento optimizado para Big Data\n",
    "try:\n",
    "    import delta\n",
    "    print(f\"Delta Lake: {delta.__version__}\")\n",
    "    print(\"‚úÖ Delta Lake disponible\")\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è Delta Lake no instalado (se usara en labs avanzados)\")\n",
    "\n",
    "# Output esperado:\n",
    "# Delta Lake: 3.1.0\n",
    "# ‚úÖ Delta Lake disponible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# === SECCI√ìN 6 ===\n",
    "## 6. Verificaci√≥n de Directorios\n",
    "\n",
    "### Explicaci√≥n Conceptual\n",
    "El proyecto tiene una estructura de directorios organizada. Verificamos que podemos acceder a los directorios de datos donde guardaremos y leeremos archivos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos os para operaciones del sistema de archivos\n",
    "import os\n",
    "\n",
    "# Definimos los directorios que deberian existir\n",
    "directorios = [\n",
    "    \"/home/jovyan/data\",           # Directorio principal de datos\n",
    "    \"/home/jovyan/data/raw\",       # Datos sin procesar\n",
    "    \"/home/jovyan/data/processed\", # Datos procesados\n",
    "    \"/home/jovyan/data/sample\",    # Datos de ejemplo\n",
    "    \"/home/jovyan/labs\",           # Notebooks\n",
    "    \"/home/jovyan/src\"             # Codigo reutilizable\n",
    "]\n",
    "\n",
    "# Verificamos cada directorio\n",
    "print(\"Verificacion de directorios:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for directorio in directorios:\n",
    "    # os.path.exists() retorna True si el directorio existe\n",
    "    existe = os.path.exists(directorio)\n",
    "    # Usamos emoji para indicar el estado\n",
    "    estado = \"‚úÖ\" if existe else \"‚ùå\"\n",
    "    print(f\"{estado} {directorio}\")\n",
    "\n",
    "# Output esperado:\n",
    "# ‚úÖ /home/jovyan/data\n",
    "# ‚úÖ /home/jovyan/data/raw\n",
    "# ... etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# === SECCI√ìN 7 ===\n",
    "## 7. Prueba de Escritura y Lectura\n",
    "\n",
    "### Explicaci√≥n Conceptual\n",
    "Verificamos que podemos escribir y leer archivos. Esto es fundamental porque en Big Data constantemente guardamos y cargamos datos de diferentes fuentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos la ruta donde guardaremos el archivo de prueba\n",
    "ruta_prueba = \"/home/jovyan/data/sample/test_check.csv\"\n",
    "\n",
    "# Guardamos el DataFrame como archivo CSV\n",
    "# .toPandas() convierte el DataFrame de Spark a Pandas (para archivos pequenos)\n",
    "# .to_csv() guarda el DataFrame de Pandas como CSV\n",
    "df.toPandas().to_csv(ruta_prueba, index=False)\n",
    "\n",
    "print(f\"Archivo guardado en: {ruta_prueba}\")\n",
    "\n",
    "# Output esperado:\n",
    "# Archivo guardado en: /home/jovyan/data/sample/test_check.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leemos el archivo CSV con Spark\n",
    "# spark.read.csv() lee archivos CSV\n",
    "# header=True indica que la primera fila son los nombres de columnas\n",
    "# inferSchema=True hace que Spark detecte automaticamente los tipos de datos\n",
    "df_leido = spark.read.csv(ruta_prueba, header=True, inferSchema=True)\n",
    "\n",
    "# Mostramos el contenido leido\n",
    "print(\"Contenido leido del archivo:\")\n",
    "df_leido.show()\n",
    "\n",
    "# Verificamos que los datos son iguales\n",
    "print(f\"Filas originales: {df.count()}\")\n",
    "print(f\"Filas leidas: {df_leido.count()}\")\n",
    "\n",
    "# Output esperado:\n",
    "# +------+----+-----------+\n",
    "# |nombre|edad|     ciudad|\n",
    "# ... (mismos datos que antes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# === SECCI√ìN 8 ===\n",
    "## 8. Resumen de Verificaci√≥n\n",
    "\n",
    "### Explicaci√≥n Conceptual\n",
    "Hacemos un resumen de todas las verificaciones para tener una vista completa del estado del entorno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos un resumen de todas las verificaciones\n",
    "print(\"=\" * 60)\n",
    "print(\"         RESUMEN DE VERIFICACI√ìN DEL ENTORNO\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "print(f\"‚úÖ Python: {sys.version.split()[0]}\")\n",
    "print(f\"‚úÖ PySpark: {pyspark.__version__}\")\n",
    "print(f\"‚úÖ Spark Session: Activa\")\n",
    "print(f\"‚úÖ DataFrame: {df.count()} filas creadas\")\n",
    "print(f\"‚úÖ Pandas: {pd.__version__}\")\n",
    "print(f\"‚úÖ NumPy: {np.__version__}\")\n",
    "print(f\"‚úÖ Lectura/Escritura: Funcionando\")\n",
    "print()\n",
    "print(\"=\" * 60)\n",
    "print(\"   üéâ ¬°Tu entorno est√° listo para Big Data!\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "print(\"Pr√≥ximo paso: Abre 02_spark_basics.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# === EJERCICIOS PR√ÅCTICOS ===\n",
    "\n",
    "### üéØ Ejercicio 1.1: Explorar la Spark UI\n",
    "\n",
    "1. Abre http://localhost:4040 en tu navegador\n",
    "2. Encuentra la pesta√±a \"Jobs\"\n",
    "3. ¬øCu√°ntos jobs se han ejecutado?\n",
    "\n",
    "**Pistas:**\n",
    "- Cada `df.count()` o `df.show()` genera al menos un job\n",
    "- La UI muestra el historial de todos los jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Escribe aqui tus observaciones de la Spark UI\n",
    "# ¬øCuantos jobs viste?\n",
    "# ¬øQue informacion te parecio interesante?\n",
    "\n",
    "# Respuesta:\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéØ Ejercicio 1.2: Crear tu propio DataFrame\n",
    "\n",
    "Crea un DataFrame con informaci√≥n de 3 productos:\n",
    "- Columnas: producto, precio, cantidad\n",
    "- Muestra el DataFrame con `show()`\n",
    "\n",
    "**Pistas:**\n",
    "- Usa `spark.createDataFrame(datos, columnas)`\n",
    "- Los datos son una lista de tuplas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Crea tu DataFrame de productos aqui\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úÖ Soluci√≥n Ejercicio 1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solucion: DataFrame de productos\n",
    "\n",
    "# Definimos los datos de productos\n",
    "productos_datos = [\n",
    "    (\"Laptop\", 15000.00, 5),\n",
    "    (\"Mouse\", 350.00, 20),\n",
    "    (\"Teclado\", 800.00, 15)\n",
    "]\n",
    "\n",
    "# Definimos los nombres de columnas\n",
    "productos_columnas = [\"producto\", \"precio\", \"cantidad\"]\n",
    "\n",
    "# Creamos el DataFrame\n",
    "df_productos = spark.createDataFrame(productos_datos, productos_columnas)\n",
    "\n",
    "# Mostramos el resultado\n",
    "df_productos.show()\n",
    "\n",
    "# Output esperado:\n",
    "# +--------+-------+--------+\n",
    "# |producto| precio|cantidad|\n",
    "# +--------+-------+--------+\n",
    "# |  Laptop|15000.0|       5|\n",
    "# |   Mouse|  350.0|      20|\n",
    "# | Teclado|  800.0|      15|\n",
    "# +--------+-------+--------+"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéØ Ejercicio 1.3: Contar y Filtrar\n",
    "\n",
    "Usando el DataFrame de personas creado anteriormente (`df`):\n",
    "1. Cuenta cu√°ntas personas tienen m√°s de 25 a√±os\n",
    "2. Muestra solo las personas de CDMX\n",
    "\n",
    "**Pistas:**\n",
    "- Usa `df.filter(condicion)` para filtrar\n",
    "- La condici√≥n se escribe como `df[\"columna\"] > valor`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Filtra el DataFrame aqui\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úÖ Soluci√≥n Ejercicio 1.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solucion: Filtrar DataFrame\n",
    "\n",
    "# 1. Personas mayores de 25 anos\n",
    "# filter() selecciona solo las filas que cumplen la condicion\n",
    "mayores_25 = df.filter(df[\"edad\"] > 25)\n",
    "\n",
    "print(f\"Personas mayores de 25: {mayores_25.count()}\")\n",
    "mayores_25.show()\n",
    "\n",
    "# 2. Personas de CDMX\n",
    "# Filtramos por igualdad de string\n",
    "de_cdmx = df.filter(df[\"ciudad\"] == \"CDMX\")\n",
    "\n",
    "print(f\"Personas de CDMX: {de_cdmx.count()}\")\n",
    "de_cdmx.show()\n",
    "\n",
    "# Output esperado:\n",
    "# Personas mayores de 25: 3\n",
    "# +------+----+-----------+\n",
    "# |nombre|edad|     ciudad|\n",
    "# +------+----+-----------+\n",
    "# |Carlos|  30|Guadalajara|\n",
    "# | Maria|  28|  Monterrey|\n",
    "# |  Juan|  35|       CDMX|\n",
    "# +------+----+-----------+\n",
    "#\n",
    "# Personas de CDMX: 2\n",
    "# +------+----+------+\n",
    "# |nombre|edad|ciudad|\n",
    "# +------+----+------+\n",
    "# |   Ana|  25|  CDMX|\n",
    "# |  Juan|  35|  CDMX|\n",
    "# +------+----+------+"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# === RESUMEN FINAL ===\n",
    "\n",
    "## Resumen\n",
    "\n",
    "### Conceptos Clave\n",
    "- **SparkSession**: Punto de entrada para usar Spark. Como \"encender\" el motor.\n",
    "- **DataFrame**: Tabla de datos distribuida. Como Excel pero para Big Data.\n",
    "- **Esquema**: Estructura de un DataFrame (columnas y tipos de datos).\n",
    "- **Acciones**: Operaciones que ejecutan c√°lculos (`count()`, `show()`, `collect()`).\n",
    "\n",
    "### Conexi√≥n con AWS\n",
    "- **Amazon EMR**: Servicio AWS que ejecuta clusters de Spark en la nube\n",
    "- **AWS Glue**: Servicio serverless que tambi√©n usa Spark internamente\n",
    "- Lo que aprendas con Spark local aplica directamente en AWS\n",
    "\n",
    "### Siguiente Paso\n",
    "Contin√∫a con: `02_spark_basics.ipynb` para aprender operaciones fundamentales de Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limpieza: Eliminamos el archivo de prueba (opcional)\n",
    "import os\n",
    "if os.path.exists(ruta_prueba):\n",
    "    os.remove(ruta_prueba)\n",
    "    print(\"Archivo de prueba eliminado\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
